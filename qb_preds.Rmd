---
title: "Fantasy Football Predictions"
author: 'Ethan Allavarpu (UID: 405287603)'
date: "1/18/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(knitr)
library(MASS)
library(alr3)
library(leaps)
library(dplyr)
```

```{r}
source("data_readin_clean.R")
```

# Outliers
```{r}
qbs <- pos_ff_pts$QB %>% filter(new_ff_scores >= 76.5)
num_vars <- vapply(qbs, is.numeric, logical(1))
qbs <- qbs[, num_vars]
qbs <- mutate(qbs, comp_pct = pass_comp / pass_att,
              pass_ypg = pass_yd / games_started,
              rush_ypg = rush_yd / games_started)
```


# Add Cluster Variables for Each Position
## Best K Clusters Function
```{r}
best_k_means <- function(data, min_k, max_k) {
  if (min_k > max_k) {
    warning("Minimum K greater than maximum K--values swapped")
    temp <- max_k
    max_k <- min_k
    min_k <- temp
  }
  ss <- numeric(max_k - min_k + 1)
  k_means_clusters <- list()
  length(k_means_clusters) <- max_k - min_k + 1
  names(k_means_clusters) <- paste("clusters_", min_k:max_k, sep = "")
  for (k in seq(from = min_k, to = max_k, by = 1)) {
    k_means <- kmeans(data, centers = k, nstart = 100, iter.max = 20)
    k_means_clusters[[k - 1]] <- k_means
    ss[k - 1] <- k_means$tot.withinss
  }
  plot(seq_along(ss) + 1, ss, ylim = c(0, range(ss)[2]))
  k_means_clusters
}

assigned_cluster <- function(newdata, clustering_object) {
  centers <- clustering_object$centers
  rel_data <- newdata %>% dplyr::select(colnames(centers))
  new_clusters <- matrix(ncol = nrow(centers), nrow = nrow(rel_data))
  for (i in seq_len(nrow(centers))) {
    ith_clust <- centers[i, ]
    sq_dist <- (ith_clust - rel_data)^2
    dist <- apply(sq_dist, 1, sum)
    new_clusters[, i] <- sqrt(dist)
  }
  apply(new_clusters, 1, which.min)
}
```

## QB: Pure passer or dual threat?
```{r}
set.seed(1)
qb_stats <- dplyr::select(qbs, age:rush_td, fmb:two_pt_pass,
                   comp_pct:rush_ypg)
qb_clusters <- best_k_means(qb_stats, 2, 25)
best_qb_clust <- qb_clusters$clusters_4
qbs <- mutate(qbs, play_style = factor(best_qb_clust$cluster))
```

## RB: Running exclusive or receiving threat?
```{r}
rbs <- pos_ff_pts$RB
rbs <- mutate(rbs, rush_ypg = rush_yd / games,
              rec_ypg = rec_yd / games)
rb_stats <- scale(dplyr::select(rbs, age:games_started, rush_att:two_pt_made,
                         rush_ypg:rec_ypg))
rb_clusters <- best_k_means(rb_stats, 2, 25)
best_rb_clust <- rb_clusters$clusters_5
rbs <- mutate(rbs, play_style = factor(best_rb_clust$cluster))
```

## WR: High-volume or catch-efficient?
```{r}
wrs <- pos_ff_pts$WR
wrs <- mutate(wrs, rush_ypg = rush_yd / games,
              rec_ypg = rec_yd / games)
wr_stats <- scale(dplyr::select(wrs, age:games_started, rush_att:two_pt_made,
                         rush_ypg:rec_ypg))
wr_clusters <- best_k_means(wr_stats, 2, 25)
best_wr_clust <- wr_clusters$clusters_3
wrs <- mutate(wrs, play_style = factor(best_wr_clust$cluster))
```

## TE: Possession or red-zone threat?
```{r}
tes <- pos_ff_pts$TE
tes <- mutate(tes, rush_ypg = rush_yd / games,
              rec_ypg = rec_yd / games)
te_stats <- scale(dplyr::select(tes, age:games_started, rush_att:two_pt_made,
                         rush_ypg:rec_ypg))
te_clusters <- best_k_means(te_stats, 2, 25)
best_te_clust <- te_clusters$clusters_5
tes <- mutate(tes, play_style = factor(best_te_clust$cluster))
```

# Models
```{r}
interaction <- function(dataset) {
  mixed <- data.frame(rep(NA, nrow(dataset)))
  k <- 0
  for (i in seq_along(dataset)) {
    for (j in seq_along(dataset)) {
      if (!any(names(dataset)[c(i, j)] %in% "new_ff_scores") &
          i < j &
          !is.character(dataset[[i]]) &
          !is.character(dataset[[j]]) &
          !is.factor(dataset[[i]]) &
          !is.factor(dataset[[j]])) {
        k <- k + 1
        ith <- dataset[[i]]
        jth <- dataset[[j]]
        mix <- ith * jth
        mixed <- cbind(mixed, mix)
        names(mixed)[k + 1] <- paste0("mix", i, j)
      }
    }
  }
  mixed
}
```


## FIRSTQB
```{r}
simple_lm <- lm(new_ff_scores ~ ppr, data = qbs)
qb_validation <- filter(ff_validation, position == "QB")
simple_preds <- predict(simple_lm, qb_validation)
simple_mse <- mean((simple_preds - qb_validation$new_ff_scores)^2)
simple_mse
qb_mses <- numeric(1)
qb_mses[1] <- simple_mse
names(qb_mses)[1] <- "basic"
```


```{r}
set.seed(2)
names(qbs)
# lm
qb_vars <- dplyr::select(qbs,
                  age:rush_td,
                  fmb:two_pt_pass,
                  comp_pct:rush_ypg,
                  ppr,
                  new_ff_scores,
                  play_style)

qb_validation <- filter(ff_validation, position == "QB") %>%
  mutate(comp_pct = pass_comp / pass_att,
         pass_ypg = pass_yd / games_started,
         rush_ypg = rush_yd / games_started)
qb_validation <- mutate(qb_validation,
                        play_style = factor(assigned_cluster(qb_validation,
                                                             best_qb_clust),
                                            levels = c("1", "2", "3", "4")))
qb_lm <- lm(new_ff_scores ~ ., data = qb_vars)
qb_preds <- predict(qb_lm, qb_validation)
val_mse <- mean((qb_validation$new_ff_scores - qb_preds)^2)
total_mse <- mean((qb_validation$new_ff_scores - mean(qb_validation$new_ff_scores))^2)
val_mse / total_mse
qb_mses[2] <- val_mse
names(qb_mses)[2] <- "mv_lm"
cor(qb_validation$new_ff_scores, qb_preds)
plot(qb_preds, qb_preds - qb_validation$new_ff_scores)

p <- ncol(qb_vars) - 1
library(randomForest)
qb_bag <- randomForest(new_ff_scores ~ ., data = qb_vars,
                           mtry = p, importance = TRUE, ntree = 50)
qb_preds <- predict(qb_bag, dplyr::select(qb_validation, names(qb_vars)))
val_mse <- mean((qb_validation$new_ff_scores - qb_preds)^2)
cor(qb_validation$new_ff_scores, qb_preds)
val_mse / total_mse
qb_mses[3] <- val_mse
names(qb_mses)[3] <- "bag"

library(glmnet)
# I will use LASSO to perform shrinkage (over ridge regression)
train_x <- model.matrix(new_ff_scores ~ ., data = qb_vars)[, -1]
train_y <- qb_vars$new_ff_scores
qb_val_rel_vars <- dplyr::select(qb_validation, age:rush_td, fmb:two_pt_pass, comp_pct:rush_ypg, ppr, new_ff_scores, play_style)
test_x <- model.matrix(new_ff_scores ~ ., data = qb_val_rel_vars)[, -1]
lambda_grid <- 10^(seq(from = 10, to = -2, length.out = 100))
salary_lasso <- glmnet(train_x, train_y, family = "gaussian",
                       alpha = 1, lambda = lambda_grid, standardize = FALSE)
salary_lasso_cv <- cv.glmnet(train_x, train_y, family = "gaussian", alpha = 1,
                             lambda = lambda_grid, standardize = FALSE,
                             nfolds = 10)
plot(salary_lasso_cv)
lambda_1se <- salary_lasso_cv$lambda.min
lambda_1se
val_preds <- predict(salary_lasso, test_x, s = lambda_1se)
val_mse <- mean((val_preds - qb_val_rel_vars$new_ff_scores)^2)
val_mse / total_mse
qb_mses[4] <- val_mse
names(qb_mses)[4] <- "lasso"
salary_lasso <- glmnet(train_x, train_y, family = "gaussian",
                       alpha = 0, lambda = lambda_grid, standardize = FALSE)
salary_lasso_cv <- cv.glmnet(train_x, train_y, family = "gaussian", alpha = 0,
                             lambda = lambda_grid, standardize = FALSE,
                             nfolds = 10)
plot(salary_lasso_cv)
lambda_1se <- salary_lasso_cv$lambda.min
lambda_1se
val_preds <- predict(salary_lasso, test_x, s = lambda_1se)
val_mse <- mean((val_preds - qb_val_rel_vars$new_ff_scores)^2)
val_mse / total_mse
qb_mses[5] <- val_mse
names(qb_mses)[5] <- "ridge"
```

```{r}
beta_mat <- as.matrix(salary_lasso$beta)
colnames(beta_mat) <- salary_lasso$lambda
beta_mat[, salary_lasso$lambda == lambda_1se]
```

```{r}
hist(qb_val_rel_vars$new_ff_scores, freq = FALSE, col = "blue", density = 20, ylim = c(0, 0.01))
hist(val_preds, freq = FALSE, col = "red", density = 30, xlim = range(val_preds, qb_val_rel_vars$new_ff_scores), add = TRUE)
cor(qb_val_rel_vars$new_ff_scores, val_preds)
plot(qb_val_rel_vars$new_ff_scores, qb_val_rel_vars$new_ff_scores - val_preds)
```

# SVM for Regression
```{r}
library(e1071)
svmfit <- svm(new_ff_scores ~ ., data = qb_vars, kernel = "radial", scale = TRUE)
preds <- predict(svmfit, qb_validation)
mean((qb_validation$new_ff_scores - preds)^2) / total_mse
```

# QB2
```{r}
set.seed(2)
names(qbs)
# lm
qb_vars <- dplyr::select(qbs,
                  age:rush_td,
                  fmb:two_pt_pass,
                  comp_pct:rush_ypg,
                  ppr,
                  new_ff_scores,
                  play_style)
qb_vars$new_ff_scores <- log(qb_vars$new_ff_scores)
qb_validation <- filter(ff_validation, position == "QB") %>%
  mutate(comp_pct = pass_comp / pass_att,
         pass_ypg = pass_yd / games_started,
         rush_ypg = rush_yd / games_started)
qb_validation$new_ff_scores <- log(qb_validation$new_ff_scores)
qb_validation <- mutate(qb_validation,
                        play_style = factor(assigned_cluster(qb_validation,
                                                             best_qb_clust),
                                            levels = c("1", "2", "3", "4")))
qb_lm <- lm(new_ff_scores ~ ., data = qb_vars)
qb_preds <- predict(qb_lm, qb_validation)
qb_preds <- exp(qb_preds)
val_mse <- mean((exp(qb_validation$new_ff_scores) - qb_preds)^2)
total_mse <- mean((exp(qb_validation$new_ff_scores) - mean(exp(qb_validation$new_ff_scores)))^2)
val_mse / total_mse
qb_mses[6] <- val_mse
names(qb_mses)[6] <- "ytrans_mv_lm"
plot(qb_preds, qb_preds - exp(qb_validation$new_ff_scores))

p <- ncol(qb_vars) - 1
library(randomForest)
qb_bag <- randomForest(new_ff_scores ~ ., data = qb_vars,
                           mtry = p, importance = TRUE, ntree = 50)
qb_preds <- predict(qb_bag, dplyr::select(qb_validation, names(qb_vars)))
val_mse <- mean((exp(qb_validation$new_ff_scores) - exp(qb_preds))^2)
val_mse / total_mse
qb_mses[7] <- val_mse
names(qb_mses)[7] <- "ytrans_bag"

library(glmnet)
# I will use LASSO to perform shrinkage (over ridge regression)
train_x <- model.matrix(new_ff_scores ~ ., data = qb_vars)[, -1]
train_y <- qb_vars$new_ff_scores
qb_val_rel_vars <- dplyr::select(qb_validation, age:rush_td, fmb:two_pt_pass, comp_pct:rush_ypg, ppr, new_ff_scores, play_style)
test_x <- model.matrix(new_ff_scores ~ ., data = qb_val_rel_vars)[, -1]
lambda_grid <- 10^(seq(from = 10, to = -2, length.out = 100))
salary_lasso <- glmnet(train_x, train_y, family = "gaussian",
                       alpha = 0, lambda = lambda_grid, standardize = FALSE)
salary_lasso_cv <- cv.glmnet(train_x, train_y, family = "gaussian", alpha = 0,
                             lambda = lambda_grid, standardize = FALSE,
                             nfolds = 10)
plot(salary_lasso_cv)
lambda_1se <- salary_lasso_cv$lambda.min
lambda_1se
val_preds <- predict(salary_lasso, test_x, s = lambda_1se)
val_mse <- mean((exp(val_preds) - exp(qb_val_rel_vars$new_ff_scores))^2)
val_mse / total_mse
qb_mses[8] <- val_mse
names(qb_mses)[8] <- "ytrans_ridge"
```

```{r}
beta_mat <- as.matrix(salary_lasso$beta)
colnames(beta_mat) <- salary_lasso$lambda
beta_mat[, salary_lasso$lambda == lambda_1se]
```

```{r}
plot(val_preds, exp(qb_val_rel_vars$new_ff_scores))

train_results <- exp(qb_vars$new_ff_scores)
train_preds <- exp(predict(salary_lasso, train_x, s = lambda_1se))
next_model <- lm(train_results ~ sqrt(train_preds))
plot(next_model)


next_preds <- next_model$coefficients[1] + next_model$coefficients[2] * sqrt(exp(val_preds))
plot(next_preds, exp(qb_val_rel_vars$new_ff_scores) - next_preds)
qb_mses[9] <- mean((next_preds - exp(qb_val_rel_vars$new_ff_scores))^2)
names(qb_mses)[9] <- "basic_neural_trans"
```

# QB3
```{r}
set.seed(2)
names(qbs)
# lm
qb_vars <- dplyr::select(qbs,
                  age:rush_td,
                  fmb:two_pt_pass,
                  comp_pct:rush_ypg,
                  ppr,
                  new_ff_scores,
                  play_style)

qb_validation <- filter(ff_validation, position == "QB") %>%
  mutate(comp_pct = pass_comp / pass_att,
         pass_ypg = pass_yd / games_started,
         rush_ypg = rush_yd / games_started)
qb_validation <- mutate(qb_validation,
                        play_style = factor(assigned_cluster(qb_validation,
                                                             best_qb_clust),
                                            levels = c("1", "2", "3", "4")))
qb_lm <- lm(new_ff_scores ~ ., data = qb_vars)

qb_preds <- predict(qb_lm, qb_validation)
val_mse <- mean((qb_validation$new_ff_scores - qb_preds)^2)
total_mse <- mean((qb_validation$new_ff_scores - mean(qb_validation$new_ff_scores))^2)
val_mse / total_mse
plot(qb_preds, qb_preds - qb_validation$new_ff_scores)

invResPlot(qb_lm,
          pch = 19,
          cex = 0.75,
          col = rgb(0, 0, 0, alpha = 0.25),
          las = 1)
summary(powerTransform(qb_lm))

qb_new_vars <- qb_vars
qb_new_vars$new_ff_scores <- (qb_new_vars$new_ff_scores)^1.21
qb_lm <- lm(new_ff_scores ~ ., data = qb_new_vars)

qb_preds <- (predict(qb_lm, qb_validation))^(1 / 1.21)
val_mse <- mean((qb_validation$new_ff_scores - qb_preds)^2)
val_mse / total_mse
plot(qb_preds, qb_preds - qb_validation$new_ff_scores)


qb_predictors <- qb_vars %>% dplyr::select(age:ppr)
summary(qb_predictors)
qb_predictors <- mutate(qb_predictors, rush_yd = rush_yd + 10,
                        rush_yd_per_att = rush_yd_per_att + 1,
                        rush_td = rush_td + 1,
                        fmb = fmb + 1,
                        fmb_lst = fmb_lst + 1,
                        two_pt_made = two_pt_made + 1,
                        two_pt_pass = two_pt_pass + 1,
                        rush_ypg = rush_ypg + 1)
qb_preds_trans <- summary(powerTransform(as.matrix(qb_predictors) ~ 1))
qb_preds_trans <- qb_preds_trans$result[, 2]
qb_new_vars <- qb_vars
qb_new_vars <- mutate(qb_new_vars, rush_yd = rush_yd + 10,
                        rush_yd_per_att = rush_yd_per_att + 1,
                        rush_td = rush_td + 1,
                        fmb = fmb + 1,
                        fmb_lst = fmb_lst + 1,
                        two_pt_made = two_pt_made + 1,
                        two_pt_pass = two_pt_pass + 1,
                        rush_ypg = rush_ypg + 1)
for (i in 1:20) {
  ith_var <- qb_new_vars[[i]]
  trans <- qb_preds_trans[i]
  if (trans == 0) {
    change <- log(ith_var)
  } else {
    change <- ith_var^trans
  }
  qb_new_vars[[i]] <- change
}
qb_lm <- lm(new_ff_scores ~ ., data = qb_new_vars)

qb_new_validation <- qb_validation
qb_new_validation <- mutate(qb_new_validation, rush_yd = rush_yd + 10,
                        rush_yd_per_att = rush_yd_per_att + 1,
                        rush_td = rush_td + 1,
                        fmb = fmb + 1,
                        fmb_lst = fmb_lst + 1,
                        two_pt_made = two_pt_made + 1,
                        two_pt_pass = two_pt_pass + 1,
                        rush_ypg = rush_ypg + 1) %>%
  dplyr::select(age:rush_td,
                  fmb:two_pt_pass,
                  comp_pct:rush_ypg,
                  ppr,
                  new_ff_scores,
                  play_style)

for (i in 1:20) {
  ith_var <- qb_new_validation[[i]]
  trans <- qb_preds_trans[i]
  if (trans == 0) {
    change <- log(ith_var)
  } else {
    change <- ith_var^trans
  }
  qb_new_validation[[i]] <- change
}
qb_preds <- predict(qb_lm, qb_validation)
val_mse <- mean((qb_validation$new_ff_scores - qb_preds)^2)
val_mse / total_mse
plot(qb_preds, qb_preds - qb_validation$new_ff_scores)

p <- ncol(qb_vars) - 1
library(randomForest)
qb_bag <- randomForest(new_ff_scores ~ ., data = qb_new_vars,
                           mtry = p, importance = TRUE, ntree = 50)
qb_preds <- predict(qb_bag, dplyr::select(qb_new_validation, names(qb_vars)))
val_mse <- mean((qb_validation$new_ff_scores - qb_preds)^2)
val_mse / total_mse

library(glmnet)
# I will use LASSO to perform shrinkage (over ridge regression)
train_x <- model.matrix(new_ff_scores ~ ., data = qb_vars)[, -1]
train_y <- qb_vars$new_ff_scores
qb_val_rel_vars <- dplyr::select(qb_validation, age:rush_td, fmb:two_pt_pass, comp_pct:rush_ypg, ppr, new_ff_scores, play_style)
test_x <- model.matrix(new_ff_scores ~ ., data = qb_val_rel_vars)[, -1]
lambda_grid <- 10^(seq(from = 10, to = -2, length.out = 100))
salary_lasso <- glmnet(train_x, train_y, family = "gaussian",
                       alpha = 1, lambda = lambda_grid, standardize = FALSE)
salary_lasso_cv <- cv.glmnet(train_x, train_y, family = "gaussian", alpha = 1,
                             lambda = lambda_grid, standardize = FALSE,
                             nfolds = 10)
plot(salary_lasso_cv)
lambda_1se <- salary_lasso_cv$lambda.min
lambda_1se
val_preds <- predict(salary_lasso, test_x, s = lambda_1se)
val_mse <- mean((val_preds - qb_val_rel_vars$new_ff_scores)^2)
val_mse / total_mse
```

```{r}
beta_mat <- as.matrix(salary_lasso$beta)
colnames(beta_mat) <- salary_lasso$lambda
beta_mat[, salary_lasso$lambda == lambda_1se]
```

```{r}
hist(qb_val_rel_vars$new_ff_scores, freq = FALSE, col = "blue", density = 20, ylim = c(0, 0.01))
hist(val_preds, freq = FALSE, col = "red", density = 30, xlim = range(val_preds, qb_val_rel_vars$new_ff_scores), add = TRUE)

plot(qb_val_rel_vars$new_ff_scores, qb_val_rel_vars$new_ff_scores - val_preds)
```

# SVM for Regression
```{r}
library(e1071)
svmfit <- svm(new_ff_scores ~ ., data = qb_vars, kernel = "radial", scale = TRUE)
preds <- predict(svmfit, qb_validation)
mean((qb_validation$new_ff_scores - preds)^2) / total_mse
```
